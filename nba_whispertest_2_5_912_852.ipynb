{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f280665d-3df1-46b2-bbc0-4ccbbc7b6da5",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-09-10T23:18:31.619588Z",
          "iopub.status.busy": "2024-09-10T23:18:31.619183Z",
          "iopub.status.idle": "2024-09-10T23:21:14.732984Z",
          "shell.execute_reply": "2024-09-10T23:21:14.731744Z",
          "shell.execute_reply.started": "2024-09-10T23:18:31.619560Z"
        },
        "colab": {
          "referenced_widgets": [
            "b5ab4e68d5da41b5893a1eef3ac356ac",
            "a5db72b09ab14e16b162028a51c1b169",
            "4601e6f2c96343b2b3d227de508a578a",
            "571fc421a91a4d80b2f8f9ce74b24c7f",
            "91da73e982bf48e8b08648bcff679608",
            "f452f4080144499bb6ad39e6853fa328",
            "419a1990777c417a9ab8cb6a00671f31",
            "0896abfbcb9347debde5afd5bbe91a6a"
          ]
        },
        "id": "f280665d-3df1-46b2-bbc0-4ccbbc7b6da5",
        "outputId": "22c0ed98-aae8-4b38-aa48-957111dfef4c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.35.2)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.44.2-py3-none-any.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.13.1)\n",
            "Collecting huggingface-hub<1.0,>=0.23.2 (from transformers)\n",
            "  Downloading huggingface_hub-0.24.6-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from transformers) (5.4.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.31.0)\n",
            "Collecting safetensors>=0.4.1 (from transformers)\n",
            "  Downloading safetensors-0.4.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
            "Collecting tokenizers<0.20,>=0.19 (from transformers)\n",
            "  Downloading tokenizers-0.19.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->transformers) (3.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Downloading transformers-4.44.2-py3-none-any.whl (9.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading huggingface_hub-0.24.6-py3-none-any.whl (417 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m417.5/417.5 kB\u001b[0m \u001b[31m46.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safetensors-0.4.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (435 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m435.0/435.0 kB\u001b[0m \u001b[31m50.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.19.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m58.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: safetensors, huggingface-hub, tokenizers, transformers\n",
            "  Attempting uninstall: safetensors\n",
            "    Found existing installation: safetensors 0.4.0\n",
            "    Uninstalling safetensors-0.4.0:\n",
            "      Successfully uninstalled safetensors-0.4.0\n",
            "  Attempting uninstall: huggingface-hub\n",
            "    Found existing installation: huggingface-hub 0.20.3\n",
            "    Uninstalling huggingface-hub-0.20.3:\n",
            "      Successfully uninstalled huggingface-hub-0.20.3\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.15.1\n",
            "    Uninstalling tokenizers-0.15.1:\n",
            "      Successfully uninstalled tokenizers-0.15.1\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.35.2\n",
            "    Uninstalling transformers-4.35.2:\n",
            "      Successfully uninstalled transformers-4.35.2\n",
            "Successfully installed huggingface-hub-0.24.6 safetensors-0.4.5 tokenizers-0.19.1 transformers-4.44.2\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mLooking in indexes: https://download.pytorch.org/whl/cu121\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.1.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.16.1+cu121)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.1.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from torch) (4.9.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (2.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (1.26.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.31.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (9.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (2.1.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->torchvision) (3.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->torchvision) (2020.6.20)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.11/dist-packages (from sympy->torch) (1.3.0)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mLooking in indexes: https://pypi.org/simple, https://abetlen.github.io/llama-cpp-python/whl/cu121\n",
            "Collecting llama-cpp-python\n",
            "  Downloading https://github.com/abetlen/llama-cpp-python/releases/download/v0.2.90-cu121/llama_cpp_python-0.2.90-cp311-cp311-linux_x86_64.whl (448.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m448.2/448.2 MB\u001b[0m \u001b[31m196.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting typing-extensions>=4.5.0 (from llama-cpp-python)\n",
            "  Downloading typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting numpy>=1.20.0 (from llama-cpp-python)\n",
            "  Downloading numpy-2.1.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.9/60.9 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting diskcache>=5.6.1 (from llama-cpp-python)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting jinja2>=2.11.3 (from llama-cpp-python)\n",
            "  Downloading jinja2-3.1.4-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting MarkupSafe>=2.0 (from jinja2>=2.11.3->llama-cpp-python)\n",
            "  Downloading MarkupSafe-2.1.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jinja2-3.1.4-py3-none-any.whl (133 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.3/133.3 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-2.1.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.3/16.3 MB\u001b[0m \u001b[31m67.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
            "Downloading MarkupSafe-2.1.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (28 kB)\n",
            "Installing collected packages: typing-extensions, numpy, MarkupSafe, diskcache, jinja2, llama-cpp-python\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.9.0\n",
            "    Uninstalling typing_extensions-4.9.0:\n",
            "      Successfully uninstalled typing_extensions-4.9.0\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.3\n",
            "    Uninstalling numpy-1.26.3:\n",
            "      Successfully uninstalled numpy-1.26.3\n",
            "  Attempting uninstall: MarkupSafe\n",
            "    Found existing installation: MarkupSafe 2.1.4\n",
            "    Uninstalling MarkupSafe-2.1.4:\n",
            "      Successfully uninstalled MarkupSafe-2.1.4\n",
            "  Attempting uninstall: jinja2\n",
            "    Found existing installation: Jinja2 3.1.3\n",
            "    Uninstalling Jinja2-3.1.3:\n",
            "      Successfully uninstalled Jinja2-3.1.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "pywavelets 1.5.0 requires numpy<2.0,>=1.22.4, but you have numpy 2.1.1 which is incompatible.\n",
            "scipy 1.11.2 requires numpy<1.28.0,>=1.21.6, but you have numpy 2.1.1 which is incompatible.\n",
            "cupy-cuda12x 12.2.0 requires numpy<1.27,>=1.20, but you have numpy 2.1.1 which is incompatible.\n",
            "gradient 2.0.6 requires attrs<=19, but you have attrs 23.1.0 which is incompatible.\n",
            "matplotlib 3.7.3 requires numpy<2,>=1.20, but you have numpy 2.1.1 which is incompatible.\n",
            "contourpy 1.2.0 requires numpy<2.0,>=1.20, but you have numpy 2.1.1 which is incompatible.\n",
            "pandas 2.2.0 requires numpy<2,>=1.23.2; python_version == \"3.11\", but you have numpy 2.1.1 which is incompatible.\n",
            "tensorflow 2.15.0 requires numpy<2.0.0,>=1.23.5, but you have numpy 2.1.1 which is incompatible.\n",
            "pyarrow 15.0.0 requires numpy<2,>=1.16.6, but you have numpy 2.1.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed MarkupSafe-2.1.5 diskcache-5.6.3 jinja2-3.1.4 llama-cpp-python-0.2.90 numpy-2.1.1 typing-extensions-4.12.2\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b5ab4e68d5da41b5893a1eef3ac356ac",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "llama-3-sqlcoder-8b-Q5_K_M.gguf:   0%|          | 0.00/5.73G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "llama_model_loader: loaded meta data with 26 key-value pairs and 291 tensors from /root/.cache/huggingface/hub/models--bartowski--llama-3-sqlcoder-8b-GGUF/snapshots/57201bd362a1270614a15458f7aa3c3ed7826b6e/llama-3-sqlcoder-8b-Q5_K_M.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = llama-3-sqlcoder-8b\n",
            "llama_model_loader: - kv   2:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv   3:                       llama.context_length u32              = 8192\n",
            "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
            "llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000\n",
            "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  10:                          general.file_type u32              = 17\n",
            "llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256\n",
            "llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/root/.cache/huggingface/hub/models--bartowski--llama-3-sqlcoder-8b-GGUF/snapshots/57201bd362a1270614a15458f7aa3c3ed7826b6e/llama-3-sqlcoder-8b-Q5_K_M.gguf\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
            "llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000\n",
            "llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009\n",
            "llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...\n",
            "llama_model_loader: - kv  21:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - kv  22:                      quantize.imatrix.file str              = /models/llama-3-sqlcoder-8b-GGUF/llam...\n",
            "llama_model_loader: - kv  23:                   quantize.imatrix.dataset str              = /training_data/calibration_data.txt\n",
            "llama_model_loader: - kv  24:             quantize.imatrix.entries_count i32              = 224\n",
            "llama_model_loader: - kv  25:              quantize.imatrix.chunks_count i32              = 189\n",
            "llama_model_loader: - type  f32:   65 tensors\n",
            "llama_model_loader: - type q5_K:  193 tensors\n",
            "llama_model_loader: - type q6_K:   33 tensors\n",
            "llm_load_vocab: special tokens cache size = 256\n",
            "llm_load_vocab: token to piece cache size = 0.8000 MB\n",
            "llm_load_print_meta: format           = GGUF V3 (latest)\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = BPE\n",
            "llm_load_print_meta: n_vocab          = 128256\n",
            "llm_load_print_meta: n_merges         = 280147\n",
            "llm_load_print_meta: vocab_only       = 0\n",
            "llm_load_print_meta: n_ctx_train      = 8192\n",
            "llm_load_print_meta: n_embd           = 4096\n",
            "llm_load_print_meta: n_layer          = 32\n",
            "llm_load_print_meta: n_head           = 32\n",
            "llm_load_print_meta: n_head_kv        = 8\n",
            "llm_load_print_meta: n_rot            = 128\n",
            "llm_load_print_meta: n_swa            = 0\n",
            "llm_load_print_meta: n_embd_head_k    = 128\n",
            "llm_load_print_meta: n_embd_head_v    = 128\n",
            "llm_load_print_meta: n_gqa            = 4\n",
            "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
            "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 14336\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: causal attn      = 1\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 0\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 500000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 0\n",
            "llm_load_print_meta: ssm_d_inner      = 0\n",
            "llm_load_print_meta: ssm_d_state      = 0\n",
            "llm_load_print_meta: ssm_dt_rank      = 0\n",
            "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
            "llm_load_print_meta: model type       = 8B\n",
            "llm_load_print_meta: model ftype      = Q5_K - Medium\n",
            "llm_load_print_meta: model params     = 8.03 B\n",
            "llm_load_print_meta: model size       = 5.33 GiB (5.70 BPW) \n",
            "llm_load_print_meta: general.name     = llama-3-sqlcoder-8b\n",
            "llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\n",
            "llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'\n",
            "llm_load_print_meta: LF token         = 128 'Ä'\n",
            "llm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\n",
            "llm_load_print_meta: max token length = 256\n",
            "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    yes\n",
            "ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\n",
            "ggml_cuda_init: found 1 CUDA devices:\n",
            "  Device 0: Quadro P5000, compute capability 6.1, VMM: yes\n",
            "llm_load_tensors: ggml ctx size =    0.27 MiB\n",
            "llm_load_tensors: offloading 32 repeating layers to GPU\n",
            "llm_load_tensors: offloading non-repeating layers to GPU\n",
            "llm_load_tensors: offloaded 33/33 layers to GPU\n",
            "llm_load_tensors:        CPU buffer size =   344.44 MiB\n",
            "llm_load_tensors:      CUDA0 buffer size =  5115.49 MiB\n",
            ".........................................................................................\n",
            "llama_new_context_with_model: n_ctx      = 4000\n",
            "llama_new_context_with_model: n_batch    = 512\n",
            "llama_new_context_with_model: n_ubatch   = 512\n",
            "llama_new_context_with_model: flash_attn = 0\n",
            "llama_new_context_with_model: freq_base  = 500000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "llama_kv_cache_init:      CUDA0 KV buffer size =   500.00 MiB\n",
            "llama_new_context_with_model: KV self size  =  500.00 MiB, K (f16):  250.00 MiB, V (f16):  250.00 MiB\n",
            "llama_new_context_with_model:  CUDA_Host  output buffer size =     0.49 MiB\n",
            "llama_new_context_with_model:      CUDA0 compute buffer size =   289.82 MiB\n",
            "llama_new_context_with_model:  CUDA_Host compute buffer size =    15.82 MiB\n",
            "llama_new_context_with_model: graph nodes  = 1030\n",
            "llama_new_context_with_model: graph splits = 2\n",
            "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
            "Model metadata: {'quantize.imatrix.entries_count': '224', 'quantize.imatrix.dataset': '/training_data/calibration_data.txt', 'quantize.imatrix.chunks_count': '189', 'quantize.imatrix.file': '/models/llama-3-sqlcoder-8b-GGUF/llama-3-sqlcoder-8b.imatrix', 'tokenizer.chat_template': \"{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}{% endif %}\", 'tokenizer.ggml.eos_token_id': '128009', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'gpt2', 'general.architecture': 'llama', 'llama.rope.freq_base': '500000.000000', 'tokenizer.ggml.pre': 'llama-bpe', 'llama.context_length': '8192', 'general.name': 'llama-3-sqlcoder-8b', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'tokenizer.ggml.bos_token_id': '128000', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8', 'general.file_type': '17', 'llama.vocab_size': '128256', 'llama.rope.dimension_count': '128'}\n",
            "Available chat formats from metadata: chat_template.default\n",
            "Guessed chat format: llama-3\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting faster-whisper\n",
            "  Downloading faster_whisper-1.0.3-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting av<13,>=11.0 (from faster-whisper)\n",
            "  Downloading av-12.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.6 kB)\n",
            "Collecting ctranslate2<5,>=4.0 (from faster-whisper)\n",
            "  Downloading ctranslate2-4.4.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: huggingface-hub>=0.13 in /usr/local/lib/python3.11/dist-packages (from faster-whisper) (0.24.6)\n",
            "Requirement already satisfied: tokenizers<1,>=0.13 in /usr/local/lib/python3.11/dist-packages (from faster-whisper) (0.19.1)\n",
            "Collecting onnxruntime<2,>=1.14 (from faster-whisper)\n",
            "  Downloading onnxruntime-1.19.2-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from ctranslate2<5,>=4.0->faster-whisper) (69.0.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from ctranslate2<5,>=4.0->faster-whisper) (2.1.1)\n",
            "Requirement already satisfied: pyyaml<7,>=5.3 in /usr/lib/python3/dist-packages (from ctranslate2<5,>=4.0->faster-whisper) (5.4.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.13->faster-whisper) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.13->faster-whisper) (2023.6.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.13->faster-whisper) (23.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.13->faster-whisper) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.13->faster-whisper) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.13->faster-whisper) (4.12.2)\n",
            "Collecting coloredlogs (from onnxruntime<2,>=1.14->faster-whisper)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime<2,>=1.14->faster-whisper) (23.5.26)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime<2,>=1.14->faster-whisper) (4.23.4)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime<2,>=1.14->faster-whisper) (1.12)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime<2,>=1.14->faster-whisper)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.13->faster-whisper) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->huggingface-hub>=0.13->faster-whisper) (3.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.13->faster-whisper) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->huggingface-hub>=0.13->faster-whisper) (2020.6.20)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime<2,>=1.14->faster-whisper) (1.3.0)\n",
            "Downloading faster_whisper-1.0.3-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m36.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading av-12.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.4/34.4 MB\u001b[0m \u001b[31m50.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading ctranslate2-4.4.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (37.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.4/37.4 MB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
            "\u001b[?25hDownloading onnxruntime-1.19.2-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (13.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.2/13.2 MB\u001b[0m \u001b[31m77.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: humanfriendly, ctranslate2, av, coloredlogs, onnxruntime, faster-whisper\n",
            "Successfully installed av-12.3.0 coloredlogs-15.0.1 ctranslate2-4.4.0 faster-whisper-1.0.3 humanfriendly-10.0 onnxruntime-1.19.2\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "A module that was compiled using NumPy 1.x cannot be run in\n",
            "NumPy 2.1.1 as it may crash. To support both 1.x and 2.x\n",
            "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
            "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
            "\n",
            "If you are a user of the module, the easiest solution will be to\n",
            "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
            "We expect that some modules will need time to support NumPy 2.\n",
            "\n",
            "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
            "  File \"<frozen runpy>\", line 88, in _run_code\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel_launcher.py\", line 17, in <module>\n",
            "    app.launch_new_instance()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelapp.py\", line 739, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tornado/platform/asyncio.py\", line 205, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 607, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 1922, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.11/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 542, in dispatch_queue\n",
            "    await self.process_one()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 531, in process_one\n",
            "    await dispatch(*args)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
            "    await result\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py\", line 359, in execute_request\n",
            "    await super().execute_request(stream, ident, parent)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 775, in execute_request\n",
            "    reply_content = await reply_content\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py\", line 446, in do_execute\n",
            "    res = shell.run_cell(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
            "    return super().run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3051, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3106, in _run_cell\n",
            "    result = runner(coro)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3311, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3493, in run_ast_nodes\n",
            "    if await self.run_code(code, result, async_=asy):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"/tmp/ipykernel_42/3060270999.py\", line 50, in <module>\n",
            "    import torch\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/__init__.py\", line 1382, in <module>\n",
            "    from .functional import *  # noqa: F403\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/functional.py\", line 7, in <module>\n",
            "    import torch.nn.functional as F\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/__init__.py\", line 1, in <module>\n",
            "    from .modules import *  # noqa: F403\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/__init__.py\", line 35, in <module>\n",
            "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py\", line 20, in <module>\n",
            "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:84.)\n",
            "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "12.1\n",
            "Quadro P5000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a5db72b09ab14e16b162028a51c1b169",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4601e6f2c96343b2b3d227de508a578a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/2.48M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "571fc421a91a4d80b2f8f9ce74b24c7f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocabulary.json:   0%|          | 0.00/1.07M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "91da73e982bf48e8b08648bcff679608",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "preprocessor_config.json:   0%|          | 0.00/340 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f452f4080144499bb6ad39e6853fa328",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/2.39k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "419a1990777c417a9ab8cb6a00671f31",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.bin:   0%|          | 0.00/3.09G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting sqlite-utils\n",
            "  Downloading sqlite_utils-3.37-py3-none-any.whl.metadata (7.6 kB)\n",
            "Collecting sqlite-fts4 (from sqlite-utils)\n",
            "  Downloading sqlite_fts4-1.0.3-py3-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from sqlite-utils) (8.1.7)\n",
            "Collecting click-default-group>=1.2.3 (from sqlite-utils)\n",
            "  Downloading click_default_group-1.2.4-py2.py3-none-any.whl.metadata (2.8 kB)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.11/dist-packages (from sqlite-utils) (0.9.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.11/dist-packages (from sqlite-utils) (2.8.2)\n",
            "Collecting pluggy (from sqlite-utils)\n",
            "  Downloading pluggy-1.5.0-py3-none-any.whl.metadata (4.8 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil->sqlite-utils) (1.16.0)\n",
            "Downloading sqlite_utils-3.37-py3-none-any.whl (67 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.7/67.7 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading click_default_group-1.2.4-py2.py3-none-any.whl (4.1 kB)\n",
            "Downloading pluggy-1.5.0-py3-none-any.whl (20 kB)\n",
            "Downloading sqlite_fts4-1.0.3-py3-none-any.whl (10.0 kB)\n",
            "Installing collected packages: sqlite-fts4, pluggy, click-default-group, sqlite-utils\n",
            "Successfully installed click-default-group-1.2.4 pluggy-1.5.0 sqlite-fts4-1.0.3 sqlite-utils-3.37\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\n",
            "A module that was compiled using NumPy 1.x cannot be run in\n",
            "NumPy 2.1.1 as it may crash. To support both 1.x and 2.x\n",
            "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
            "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
            "\n",
            "If you are a user of the module, the easiest solution will be to\n",
            "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
            "We expect that some modules will need time to support NumPy 2.\n",
            "\n",
            "Traceback (most recent call last):  File \"/usr/local/bin/sqlite-utils\", line 5, in <module>\n",
            "    from sqlite_utils.cli import cli\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sqlite_utils/__init__.py\", line 4, in <module>\n",
            "    from .db import Database\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sqlite_utils/db.py\", line 73, in <module>\n",
            "    import pandas as pd  # type: ignore\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pandas/__init__.py\", line 26, in <module>\n",
            "    from pandas.compat import (\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pandas/compat/__init__.py\", line 27, in <module>\n",
            "    from pandas.compat.pyarrow import (\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pandas/compat/pyarrow.py\", line 8, in <module>\n",
            "    import pyarrow as pa\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pyarrow/__init__.py\", line 65, in <module>\n",
            "    import pyarrow.lib as _lib\n",
            "AttributeError: _ARRAY_API not found\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/sqlite-utils\", line 5, in <module>\n",
            "    from sqlite_utils.cli import cli\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sqlite_utils/__init__.py\", line 4, in <module>\n",
            "    from .db import Database\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sqlite_utils/db.py\", line 73, in <module>\n",
            "    import pandas as pd  # type: ignore\n",
            "    ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pandas/__init__.py\", line 49, in <module>\n",
            "    from pandas.core.api import (\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pandas/core/api.py\", line 1, in <module>\n",
            "    from pandas._libs import (\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pandas/_libs/__init__.py\", line 18, in <module>\n",
            "    from pandas._libs.interval import Interval\n",
            "  File \"interval.pyx\", line 1, in init pandas._libs.interval\n",
            "ValueError: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.11/dist-packages (0.41.2)\n",
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.43.3-py3-none-manylinux_2_24_x86_64.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (0.24.1)\n",
            "Collecting accelerate\n",
            "  Downloading accelerate-0.34.2-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.1.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.1.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (23.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.8)\n",
            "Requirement already satisfied: pyyaml in /usr/lib/python3/dist-packages (from accelerate) (5.4.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.24.6)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.4.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch->bitsandbytes) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->bitsandbytes) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->bitsandbytes) (3.1.4)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.11/dist-packages (from torch->bitsandbytes) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->bitsandbytes) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2020.6.20)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.11/dist-packages (from sympy->torch->bitsandbytes) (1.3.0)\n",
            "Downloading bitsandbytes-0.43.3-py3-none-manylinux_2_24_x86_64.whl (137.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.5/137.5 MB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading accelerate-0.34.2-py3-none-any.whl (324 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m324.4/324.4 kB\u001b[0m \u001b[31m40.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: bitsandbytes, accelerate\n",
            "  Attempting uninstall: bitsandbytes\n",
            "    Found existing installation: bitsandbytes 0.41.2\n",
            "    Uninstalling bitsandbytes-0.41.2:\n",
            "      Successfully uninstalled bitsandbytes-0.41.2\n",
            "  Attempting uninstall: accelerate\n",
            "    Found existing installation: accelerate 0.24.1\n",
            "    Uninstalling accelerate-0.24.1:\n",
            "      Successfully uninstalled accelerate-0.24.1\n",
            "Successfully installed accelerate-0.34.2 bitsandbytes-0.43.3\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mThe token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
            "Token is valid (permission: fineGrained).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0896abfbcb9347debde5afd5bbe91a6a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "gemma-2b-it.Q8_0.gguf:   0%|          | 0.00/2.67G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "llama_model_loader: loaded meta data with 24 key-value pairs and 164 tensors from /root/.cache/huggingface/hub/models--MaziyarPanahi--gemma-2b-it-GGUF/snapshots/72164ae6fc4003cecf37bc07f3a825b8a20b8cbb/gemma-2b-it.Q8_0.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = gemma\n",
            "llama_model_loader: - kv   1:                               general.name str              = models--google--gemma-2b-it\n",
            "llama_model_loader: - kv   2:                       gemma.context_length u32              = 8192\n",
            "llama_model_loader: - kv   3:                     gemma.embedding_length u32              = 2048\n",
            "llama_model_loader: - kv   4:                          gemma.block_count u32              = 18\n",
            "llama_model_loader: - kv   5:                  gemma.feed_forward_length u32              = 16384\n",
            "llama_model_loader: - kv   6:                 gemma.attention.head_count u32              = 8\n",
            "llama_model_loader: - kv   7:              gemma.attention.head_count_kv u32              = 1\n",
            "llama_model_loader: - kv   8:     gemma.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
            "llama_model_loader: - kv   9:                 gemma.attention.key_length u32              = 256\n",
            "llama_model_loader: - kv  10:               gemma.attention.value_length u32              = 256\n",
            "llama_model_loader: - kv  11:                          general.file_type u32              = 7\n",
            "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/root/.cache/huggingface/hub/models--MaziyarPanahi--gemma-2b-it-GGUF/snapshots/72164ae6fc4003cecf37bc07f3a825b8a20b8cbb/gemma-2b-it.Q8_0.gguf\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,256000]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 2, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 2\n",
            "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 1\n",
            "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 3\n",
            "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0\n",
            "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\n",
            "llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\n",
            "llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...\n",
            "llama_model_loader: - kv  23:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   37 tensors\n",
            "llama_model_loader: - type q8_0:  127 tensors\n",
            "llm_load_vocab: special tokens cache size = 4\n",
            "llm_load_vocab: token to piece cache size = 1.6014 MB\n",
            "llm_load_print_meta: format           = GGUF V3 (latest)\n",
            "llm_load_print_meta: arch             = gemma\n",
            "llm_load_print_meta: vocab type       = SPM\n",
            "llm_load_print_meta: n_vocab          = 256000\n",
            "llm_load_print_meta: n_merges         = 0\n",
            "llm_load_print_meta: vocab_only       = 0\n",
            "llm_load_print_meta: n_ctx_train      = 8192\n",
            "llm_load_print_meta: n_embd           = 2048\n",
            "llm_load_print_meta: n_layer          = 18\n",
            "llm_load_print_meta: n_head           = 8\n",
            "llm_load_print_meta: n_head_kv        = 1\n",
            "llm_load_print_meta: n_rot            = 256\n",
            "llm_load_print_meta: n_swa            = 0\n",
            "llm_load_print_meta: n_embd_head_k    = 256\n",
            "llm_load_print_meta: n_embd_head_v    = 256\n",
            "llm_load_print_meta: n_gqa            = 8\n",
            "llm_load_print_meta: n_embd_k_gqa     = 256\n",
            "llm_load_print_meta: n_embd_v_gqa     = 256\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 16384\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: causal attn      = 1\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 2\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 10000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 0\n",
            "llm_load_print_meta: ssm_d_inner      = 0\n",
            "llm_load_print_meta: ssm_d_state      = 0\n",
            "llm_load_print_meta: ssm_dt_rank      = 0\n",
            "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
            "llm_load_print_meta: model type       = 2B\n",
            "llm_load_print_meta: model ftype      = Q8_0\n",
            "llm_load_print_meta: model params     = 2.51 B\n",
            "llm_load_print_meta: model size       = 2.48 GiB (8.50 BPW) \n",
            "llm_load_print_meta: general.name     = models--google--gemma-2b-it\n",
            "llm_load_print_meta: BOS token        = 2 '<bos>'\n",
            "llm_load_print_meta: EOS token        = 1 '<eos>'\n",
            "llm_load_print_meta: UNK token        = 3 '<unk>'\n",
            "llm_load_print_meta: PAD token        = 0 '<pad>'\n",
            "llm_load_print_meta: LF token         = 227 '<0x0A>'\n",
            "llm_load_print_meta: EOT token        = 107 '<end_of_turn>'\n",
            "llm_load_print_meta: max token length = 93\n",
            "llm_load_tensors: ggml ctx size =    0.15 MiB\n",
            "llm_load_tensors: offloading 18 repeating layers to GPU\n",
            "llm_load_tensors: offloading non-repeating layers to GPU\n",
            "llm_load_tensors: offloaded 19/19 layers to GPU\n",
            "llm_load_tensors:        CPU buffer size =   531.25 MiB\n",
            "llm_load_tensors:      CUDA0 buffer size =  2539.66 MiB\n",
            ".............................................................\n",
            "llama_new_context_with_model: n_ctx      = 1024\n",
            "llama_new_context_with_model: n_batch    = 512\n",
            "llama_new_context_with_model: n_ubatch   = 512\n",
            "llama_new_context_with_model: flash_attn = 0\n",
            "llama_new_context_with_model: freq_base  = 10000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "llama_kv_cache_init:      CUDA0 KV buffer size =    18.00 MiB\n",
            "llama_new_context_with_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB\n",
            "llama_new_context_with_model:  CUDA_Host  output buffer size =     0.98 MiB\n",
            "llama_new_context_with_model:      CUDA0 compute buffer size =   504.00 MiB\n",
            "llama_new_context_with_model:  CUDA_Host compute buffer size =     6.01 MiB\n",
            "llama_new_context_with_model: graph nodes  = 601\n",
            "llama_new_context_with_model: graph splits = 2\n",
            "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
            "Model metadata: {'tokenizer.chat_template': \"{{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\\n' + message['content'] | trim + '<end_of_turn>\\n' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\\n'}}{% endif %}\", 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.unknown_token_id': '3', 'tokenizer.ggml.eos_token_id': '1', 'tokenizer.ggml.bos_token_id': '2', 'general.architecture': 'gemma', 'gemma.feed_forward_length': '16384', 'tokenizer.ggml.add_bos_token': 'true', 'gemma.attention.head_count': '8', 'general.name': 'models--google--gemma-2b-it', 'gemma.context_length': '8192', 'gemma.embedding_length': '2048', 'gemma.block_count': '18', 'gemma.attention.head_count_kv': '1', 'gemma.attention.key_length': '256', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'gemma.attention.layer_norm_rms_epsilon': '0.000001', 'gemma.attention.value_length': '256', 'general.file_type': '7'}\n",
            "Available chat formats from metadata: chat_template.default\n",
            "Using gguf chat template: {{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\n",
            "' + message['content'] | trim + '<end_of_turn>\n",
            "' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\n",
            "'}}{% endif %}\n",
            "Using chat eos_token: <eos>\n",
            "Using chat bos_token: <bos>\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting gTTS\n",
            "  Downloading gTTS-2.5.3-py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.11/dist-packages (from gTTS) (2.31.0)\n",
            "Requirement already satisfied: click<8.2,>=7.1 in /usr/local/lib/python3.11/dist-packages (from gTTS) (8.1.7)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->gTTS) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests<3,>=2.27->gTTS) (3.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->gTTS) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests<3,>=2.27->gTTS) (2020.6.20)\n",
            "Downloading gTTS-2.5.3-py3-none-any.whl (29 kB)\n",
            "Installing collected packages: gTTS\n",
            "Successfully installed gTTS-2.5.3\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# CODE: WRITTEN BY AYUSH TALUKDER, MCPS BLAIR HIGH SCHOOL\n",
        "# VERSION 1: AUGUST 2024\n",
        "##CODE FOR S2S (VOICE-TO-VOICE) APP FOR NBA GAMES INSIGHTS\n",
        "# APP DOES SPOKEN USER INTERACTION TO USER QUERIES ABOUT NBA GAMES, STATS, TRENDS, AND MORE\n",
        "# CODE EMPLOYS THE FOLLOWING PIPELINE\n",
        "# SPEECH TO TEXT TO TRANSCRIBE SPOKEN USER QUERIES (WHISPER)\n",
        "# A SQL DATABASE OF NBA STATISTIC AND CODE TO QUERY SQL DB\n",
        "# AN ADVANCED TEXT TO SQL LLM FOR CONVERTING TEXT TO SQL\n",
        "# AN SLM TO CONVERT THE SQL RESULTS TO A NARRATIVE STYLE , FOR HUMANS TO UNDERSTAND THE ANSWERS IN A MORE NATUARALLY CONSUMABLE MANNER\n",
        "# TTS TO CONVERT THE FINAL LLM RESPONSE TO THE USER QUERY INTOTO SPEECH\n",
        "# CUSTOM DATA PRE AND POST PROCESSING FOR DATA NORMALIZATION ACROSS MULTIPLE STEPS IN PIPELINE\n",
        "\n",
        "#!pip install transformers\n",
        "!pip install --upgrade transformers\n",
        "\n",
        "# WORKS WITH P5000 - POTENTIAL PROBLEM WITH A4000\n",
        "\n",
        "\n",
        "# llamacpp python install for reading in llama3.1 8B Q5 MODEL\n",
        "# THIS SHOULD WORK FOR A GPU INSTALL. BUT LOOKS LIKE CPU ONLY INSTALL IN THE PAPERSPACE CLOUD - WORKS WELL BUT SLOW\n",
        "!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
        "\n",
        "!pip install llama-cpp-python --force-reinstall --upgrade --no-cache-dir  \\\n",
        "  --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu121\n",
        "\n",
        "# NOW DOWNLOADING LLAMA TEXT2SQL COODER 8B GGUF MODEL FROM HUGGINGFACE\n",
        "from huggingface_hub import hf_hub_download\n",
        "import joblib\n",
        "\n",
        "REPO_ID = \"bartowski/llama-3-sqlcoder-8b-GGUF\"  #\"MatrixIA/llama-3-sqlcoder-8b\" #\"YOUR_REPO_ID\"\n",
        "FILENAME = \"llama-3-sqlcoder-8b-Q5_K_M.gguf\" #\"ggml-model-q4_0.gguf\"\n",
        "\n",
        "\n",
        "downloaded_model_path = hf_hub_download(repo_id=REPO_ID,filename=FILENAME) #, use_auth_token=True)\n",
        "print(downloaded_model_path)\n",
        "\n",
        "from llama_cpp import Llama\n",
        "\n",
        "llmsql = Llama(\n",
        "      model_path=downloaded_model_path,\n",
        "       n_gpu_layers=60, #-1, # Uncomment to use GPU acceleration\n",
        "       seed=1337, # Uncomment to set a specific seed\n",
        "       n_ctx=4000, # Uncomment to increase the context window\n",
        ")\n",
        "\n",
        "\n",
        "#INSTALL WHISPERER FOR CONVERTING SPEECH TO TEXT- ANY OF 99 LANGUAGES\n",
        "!pip install faster-whisper\n",
        "\n",
        "\n",
        "import torch\n",
        "print(torch.version.cuda)\n",
        "\n",
        "torch.cuda.is_available()\n",
        "print(torch.cuda.get_device_name(0))\n",
        "\n",
        "\n",
        "## INSTALL WHISPER SPEECH TO TEXT MODEL\n",
        "from faster_whisper import WhisperModel\n",
        "\n",
        "# INSTALL MODEL LARGE V3\n",
        "model_size = \"large-v3\"\n",
        "\n",
        "# Run on GPU with FP16\n",
        "#model = WhisperModel(model_size, device=\"cuda\", compute_type=\"float16\")\n",
        "modelwhisper = WhisperModel(model_size, device=\"cuda\", compute_type=\"int8\")\n",
        "\n",
        "\n",
        "# TOOL TO CONVERT CSV TO SQLITE\n",
        "!pip install sqlite-utils\n",
        "\n",
        "#!sqlite-utils insert creatures.db creatures creatures.csv --csv --detect-types\n",
        "\n",
        "!sqlite-utils insert nbatest.db nbatest \"nbagame23.csv\" --csv --detect-types #\n",
        "\n",
        "\n",
        "### INSTALL GEMMA 2 LLM TO CONVERT SQL ANSWER TO A MORE HUMAN CONSUMABLE FORMAT\n",
        "!pip install --upgrade bitsandbytes accelerate\n",
        "#!pip install --upgrade transformers\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "\n",
        "#AT GEMMA2 TOKEN\n",
        "# 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\n",
        "from huggingface_hub import login\n",
        "login(token = \"hf_XXX12345\") #put in your hf token there to run\n",
        "\n",
        "\n",
        "REPO_ID2 = \"MaziyarPanahi/gemma-2b-it-GGUF\"  #\"MatrixIA/llama-3-sqlcoder-8b\" #\"YOUR_REPO_ID\"\n",
        "FILENAME2 = \"gemma-2b-it.Q8_0.gguf\" #\"ggml-model-q4_0.gguf\"\n",
        "\n",
        "\n",
        "downloaded_model_path2 = hf_hub_download(repo_id=REPO_ID2,filename=FILENAME2) #, use_auth_token=True)\n",
        "print(downloaded_model_path2)\n",
        "\n",
        "\n",
        "llm = Llama(\n",
        "      model_path=downloaded_model_path2,\n",
        "       n_gpu_layers=60, #-1, # Uncomment to use GPU acceleration\n",
        "       seed=1337, # Uncomment to set a specific seed\n",
        "       n_ctx=1000, # Uncomment to increase the context window\n",
        ")\n",
        "\n",
        "\n",
        "###INSTALL GTTS TEXT TO SPEECH\n",
        "!pip install gTTS\n",
        "\n",
        "## SAVE THE LLM RESPONSE INTO AN MP3 AUDIO FILE\n",
        "#print(finalcleannballmresponse)\n",
        "from gtts import gTTS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f9e670a3-1c8e-478a-82c2-f250ac861142",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-09-10T23:21:14.738985Z",
          "iopub.status.busy": "2024-09-10T23:21:14.738063Z",
          "iopub.status.idle": "2024-09-10T23:21:20.759456Z",
          "shell.execute_reply": "2024-09-10T23:21:20.758760Z",
          "shell.execute_reply.started": "2024-09-10T23:21:14.738929Z"
        },
        "id": "f9e670a3-1c8e-478a-82c2-f250ac861142",
        "outputId": "3baa75b8-8e61-44fe-c5ef-e0b6ac105a34"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "llama_print_timings:        load time =    3134.02 ms\n",
            "llama_print_timings:      sample time =       4.41 ms /    42 runs   (    0.10 ms per token,  9530.29 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4237.45 ms /  1142 tokens (    3.71 ms per token,   269.50 tokens per second)\n",
            "llama_print_timings:        eval time =    1648.33 ms /    41 runs   (   40.20 ms per token,    24.87 tokens per second)\n",
            "llama_print_timings:       total time =    5967.90 ms /  1183 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'id': 'cmpl-3bb5afd6-ec29-440d-bd14-1fbdda27e710', 'object': 'text_completion', 'created': 1726010474, 'model': '/root/.cache/huggingface/hub/models--bartowski--llama-3-sqlcoder-8b-GGUF/snapshots/57201bd362a1270614a15458f7aa3c3ed7826b6e/llama-3-sqlcoder-8b-Q5_K_M.gguf', 'choices': [{'text': \"SELECT n.team_name_home, COUNT(n.wl_home) AS losses FROM nbatest n WHERE n.wl_home = 'L' GROUP BY n.team_name_home ORDER BY losses DESC LIMIT 3;\", 'index': 0, 'logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 1142, 'completion_tokens': 41, 'total_tokens': 1183}}\n",
            "SELECT n.team_name_home, COUNT(n.wl_home) AS losses FROM nbatest n WHERE n.wl_home = 'L' GROUP BY n.team_name_home ORDER BY losses DESC LIMIT 3;\n",
            "[('nbatest',)]\n",
            "SELECT n.team_name_home, COUNT(n.wl_home) AS losses FROM nbatest n WHERE n.wl_home = 'L' GROUP BY n.team_name_home ORDER BY losses DESC LIMIT 3;\n",
            "[('Detroit Pistons', 136), ('Charlotte Hornets', 124), ('San Antonio Spurs', 120)]\n"
          ]
        }
      ],
      "source": [
        "# TEST CODE TO EVALUATE TEXT2SQL FUNCTIONS\n",
        "#user_question = \"Which four NBA teams have the lowest average defensive rebounds per game?\"\n",
        "#user_question = \"Which five NBA teams have the best average offensive rebounds per game?\"\n",
        "#user_question = \"Which ten NBA teams have the best average 3 point conversions per game?\"\n",
        "#user_question = \"Which three NBA teams have the most home team wins?\"\n",
        "#user_question = \"Which NBA team has the best defense? Best defense is defined by the highest average win-loss difference.\"\n",
        "\n",
        "##DEMO 1\n",
        "#user_question = \"Which five NBA teams are the best at rebounding?\"\n",
        "\n",
        "##DEMO 2\n",
        "#user_question = \"Which NBA team shoots better 3 pointers: the Philadelphia 76ers or the Golden State Warriors?\"\n",
        "#user_question = \"Provide an explanation for which NBA team shoots better 3 pointers: the Philadelphia 76ers or the Golden State Warriors?.\"\n",
        "\n",
        "##DEMO 3\n",
        "#user_question = \"Which NBA team shoots better 3 pointers: the Philadelphia 76ers or the Golden State Warriors?\"\n",
        "#user_question = \"Which team is the best in the NBA at scoring?.\"\n",
        "#user_question = \"Which team is the best in the NBA at scoring on a per game basis?.\"\n",
        "\n",
        "#DEMO 3-2\n",
        "#user_question = \"Which three NBA teams have the worst home team win record?\"\n",
        "user_question = \"Which three NBA teams have the worst home team win record?\"\n",
        "\n",
        "\n",
        "prompt = f\"\"\"\n",
        "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
        "\n",
        "Generate a SQL query to answer this question: `{user_question}`\n",
        "You are a data analyst and interpreter. You will be given information about a specific NBA dataset, including its columns and their meanings:\n",
        "\n",
        "  team_id_home  number:       Column Unique identifier for the NBA team\n",
        "  team_abbreviation_home text:  Column Abbreviated Name of NBA team\n",
        "  team_name_home text:       Column Full Name of NBA team\n",
        "  game_date time:                Column  Date of the game\n",
        "  wl_home text:            Column  Single game Win (W) or Loss (L) by the home team\n",
        "  fgm_home number:     Column  2 point field goals or 2 pointers succesfully made by home team\n",
        "  fga_home number:      Column  2 point field goals or 2 pointers attempted by home team\n",
        "  fg_pct_home number:  Column  Succesful ratio of 2 point field goals or 2 pointers made by home team\n",
        "  fg3m_home:          Column  3 point field goals or 2 pointers succesfully made by home team\n",
        "  fg3a_home:             -- 3 point field goals or 3 pointers attempted by home team\n",
        "  fg3_pct_home,:          -- Succesful ratio of 3 point field goals or 3 pointers made by home team\n",
        "  ftm_home:              -- free throws succesfully made by home team\n",
        "  fta_home:              -- free throws attempted by home team\n",
        "  ft_pct_home:           -- Succesful ratio of free throws made by home team\n",
        "  oreb_home:             -- offensive rebounds by home team\n",
        "  dreb_home,             -- defensive rebounds by home team\n",
        "  reb_home,              -- Total rebounds by home team\n",
        "  ast_home,              -- assists by home team\n",
        "  stl_home,              -- steals by home team\n",
        "  blk_home,              -- blocks by home team\n",
        "  tov_home,              -- turnovers by home team\n",
        "  pf_home,               -- personal fouls by home team\n",
        "  pts_home,              -- total points in game made by home team\n",
        "  plus_minus_home        -- points differential of game win (+) or loss (-) by home team\n",
        "\n",
        "If you cannot answer the question with the available database schema, return 'I do not know'\n",
        "\n",
        "Using this information, you will set up the most suitable SQL statements to answer questions related to the data. These SQL statements will later be executed, so they should be error-free, compatible with Sqllite syntax, and accurately respond to the questions asked. Do not express an opinion or try to explain. Return only the SQL statement. Remember your sql statement will run on SQLite so syntax should be correct. Your output should be clear and void like this output 'Here is the SQL statement to answer the question:'. Also, do not add any comment to the SQL statement which you will generate. Only return SQL statement.\n",
        "\n",
        "DDL statements:\n",
        "CREATE TABLE nbatest (\n",
        "  team_id_home  INTEGER PRIMARY KEY, -- Unique identifier for the NBA team\n",
        "  team_abbreviation_home text,   -- Abbreviated Name of NBA team\n",
        "  team_name_home VARCHAR(50),   -- Full Name of NBA team\n",
        "  game_date DATE,                 -- Date of the game\n",
        "  wl_home VARCHAR(50),                -- Single game Win (W) or Loss (L) by the home team\n",
        "  fgm_home INTEGER,     -- 2 point field goals or 2 pointers succesfully made by home team\n",
        "  fga_home INTEGER,          -- 2 point field goals or 2 pointers attempted by home team\n",
        "  fg_pct_home DECIMAL(3,2),       -- Succesful ratio of 2 point field goals or 2 pointers made by home team\n",
        "  fg3m_home INTEGER,             -- 3 point field goals or 2 pointers succesfully made by home team\n",
        "  fg3a_home INTEGER,             -- 3 point field goals or 3 pointers attempted by home team\n",
        "  fg3_pct_home DECIMAL(3,2),          -- Succesful ratio of 3 point field goals or 3 pointers made by home team\n",
        "  ftm_home INTEGER,              -- free throws succesfully made by home team\n",
        "  fta_home INTEGER,              -- free throws attempted by home team\n",
        "  ft_pct_home DECIMAL(3,2),           -- Succesful ratio of free throws made by home team\n",
        "  oreb_home INTEGER,             -- offensive rebounds by home team\n",
        "  dreb_home INTEGER,             -- defensive rebounds by home team\n",
        "  reb_home INTEGER,              -- Total rebounds by home team\n",
        "  ast_home INTEGER,              -- assists by home team\n",
        "  stl_home INTEGER,              -- steals by home team\n",
        "  blk_home INTEGER,              -- blocks by home team\n",
        "  tov_home INTEGER,              -- turnovers by home team\n",
        "  pf_home INTEGER,               -- personal fouls by home team\n",
        "  pts_home INTEGER,              -- total points in game made by home team\n",
        "  plus_minus_home INTEGER        -- points differential of game win (+) or loss (-) by home team\n",
        ")<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
        "\n",
        "The following SQL query best answers the question `{user_question}`:\n",
        "```sql\n",
        "\"\"\"\n",
        "\n",
        "###LATEST PROMPT\n",
        "prompt = f\"\"\"\n",
        "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
        "\n",
        "Generate a SQL query to answer this question: `{user_question}`. Do not generate ANY extra output except for the SQL query,\n",
        "You are a data analyst and interpreter. You will be given information about a specific NBA dataset, including its columns and their meanings:\n",
        "\n",
        "  team_id_home  number:       Column represents Unique identifier for the NBA team\n",
        "  team_abbreviation_home text:  Column represents Abbreviated Name of NBA team\n",
        "  team_name_home text:       Column represents Full Name of NBA team\n",
        "  game_date time:                Column represents Date of the game\n",
        "  wl_home text:            Column represents  Single game Win (W) or Loss (L) by the home team\n",
        "  fgm_home number:     Column represents  2 point field goals or 2 pointers succesfully made by home team\n",
        "  fga_home number:      Column represents  2 point field goals or 2 pointers attempted by home team\n",
        "  fg_pct_home number:  Column represents  Succesful ratio of 2 point field goals or 2 pointers made by home team\n",
        "  fg3m_home:          Column represents  3 point field goals or 2 pointers succesfully made by home team\n",
        "  fg3a_home:            Column represents 3 point field goals or 3 pointers attempted by home team\n",
        "  fg3_pct_home,:      Column represents Succesful ratio of 3 point field goals or 3 pointers made by home team\n",
        "  ftm_home:           Column represents free throws succesfully made by home team\n",
        "  fta_home:            Column represents free throws attempted by home team\n",
        "  ft_pct_home:          Column represents Succesful ratio of free throws made by home team\n",
        "  oreb_home:            Column represents offensive rebounds by home team\n",
        "  dreb_home:           Column represents defensive rebounds by home team\n",
        "  reb_home:            Column represents Total rebounds by home team\n",
        "  ast_home:             Column represents assists by home team\n",
        "  stl_home:             Column represents steals by home team\n",
        "  blk_home:            Column represents blocks by home team\n",
        "  tov_home:            Column represents turnovers by home team\n",
        "  pf_home:              Column represents personal fouls by home team\n",
        "  pts_home:             Column represents total points in game made by home team\n",
        "  plus_minus_home :      Column represents points differential of game win (+) or loss (-) by home team\n",
        "\n",
        "If you cannot answer the question with the available database schema, return 'I do not know'\n",
        "\n",
        "Using this information, you will set up the most suitable SQL statements to answer questions related to the data. These SQL statements will later be executed, so they should be error-free, compatible with Sqllite syntax, and accurately respond to the questions asked. Do not express an opinion or try to explain. Return only the SQL statement. Remember your sql statement will run on SQLite so syntax should be correct. Your output should be clear and void like this output 'Here is the SQL statement to answer the question:'. Also, do not add any comment to the SQL statement which you will generate. Only return SQL statement.\n",
        "\n",
        "DDL statements:\n",
        "CREATE TABLE nbatest (\n",
        "  team_id_home  INTEGER PRIMARY KEY, -- Unique identifier for the NBA team\n",
        "  team_abbreviation_home text,   -- Abbreviated Name of NBA team\n",
        "  team_name_home VARCHAR(50),   -- Full Name of NBA team\n",
        "  game_date DATE,                 -- Date of the game\n",
        "  wl_home VARCHAR(50),                -- Single game Win (W) or Loss (L) by the home team\n",
        "  fgm_home INTEGER,     -- Combined 2 and 3 point field goals or 2 and 3 pointers succesfully made by home team\n",
        "  fga_home INTEGER,          -- Combined 2 point field goals or 2 and 3 pointers attempted by home team\n",
        "  fg_pct_home DECIMAL(3,2),       -- Succesful ratio of 2 point field goals or 2 pointers made by home team\n",
        "  fg3m_home INTEGER,             -- 3 point field goals or 2 pointers succesfully made by home team\n",
        "  fg3a_home INTEGER,             -- 3 point field goals or 3 pointers attempted by home team\n",
        "  fg3_pct_home DECIMAL(3,2),          -- Succesful ratio of 3 point field goals or 3 pointers made by home team\n",
        "  ftm_home INTEGER,              -- free throws succesfully made by home team\n",
        "  fta_home INTEGER,              -- free throws attempted by home team\n",
        "  ft_pct_home DECIMAL(3,2),           -- Succesful ratio of free throws made by home team\n",
        "  oreb_home INTEGER,             -- offensive rebounds by home team\n",
        "  dreb_home INTEGER,             -- defensive rebounds by home team\n",
        "  reb_home INTEGER,              -- Total rebounds by home team\n",
        "  ast_home INTEGER,              -- assists by home team\n",
        "  stl_home INTEGER,              -- steals by home team\n",
        "  blk_home INTEGER,              -- blocks by home team\n",
        "  tov_home INTEGER,              -- turnovers by home team\n",
        "  pf_home INTEGER,               -- personal fouls by home team\n",
        "  pts_home INTEGER,              -- total points in game made by home team\n",
        "  plus_minus_home INTEGER        -- points differential of game win (+) or loss (-) by home team\n",
        ")<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
        "\n",
        "The following SQL query best answers the question `{user_question}`:\n",
        "```sql\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "output = llmsql(\n",
        "      prompt, # Prompt\n",
        "      max_tokens=168, # Generate up to 32 tokens, set to None to generate up to the end of the context window\n",
        "      #stop=[\"Q:\", \"\\n\"], # Stop generating just before the model would generate a new question\n",
        "      #echo=True # Echo the prompt back in the output\n",
        ") # Generate a completion, can also call create_completion\n",
        "print(output)\n",
        "\n",
        "#out2=(output['choices'])\n",
        "\n",
        "#out2=(output['choices'])\n",
        "#print(out2[0]['text'])\n",
        "print(output['choices'][0]['text'])\n",
        "\n",
        "sqlexec = output['choices'][0]['text']\n",
        "\n",
        "\n",
        "import sqlite3 as lite\n",
        "\n",
        "conn = lite.connect('nbatest.db')\n",
        "cur = conn.cursor()\n",
        "\n",
        "# reading all table names\n",
        "table_list = [a for a in cur.execute(\"SELECT name FROM sqlite_master WHERE type = 'table'\")]\n",
        "# here is you table list\n",
        "print(table_list)\n",
        "\n",
        "# Be sure to close the connection\n",
        "#conn.close()\n",
        "\n",
        "sqlexec = output['choices'][0]['text']\n",
        "\n",
        "print(sqlexec)\n",
        "res=cur.execute(sqlexec)\n",
        "\n",
        "#res = cur.execute(\"SELECT name FROM sqlite_master\")\n",
        "#res.fetchall()\n",
        "\n",
        "nbasqlanswer = res.fetchall()\n",
        "print(nbasqlanswer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "79a0d11f-f6bb-45e9-843b-8832390be4e4",
      "metadata": {
        "id": "79a0d11f-f6bb-45e9-843b-8832390be4e4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8bde3d05-2a80-4fbf-aec2-b318c5a6abea",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-09-10T22:39:57.782247Z",
          "iopub.status.busy": "2024-09-10T22:39:57.781897Z",
          "iopub.status.idle": "2024-09-10T22:39:59.080550Z",
          "shell.execute_reply": "2024-09-10T22:39:59.079019Z",
          "shell.execute_reply.started": "2024-09-10T22:39:57.782211Z"
        },
        "id": "8bde3d05-2a80-4fbf-aec2-b318c5a6abea",
        "outputId": "4a82aa60-f8e5-461c-afb6-69439fcbbe94"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Detected language 'en' with probability 0.981320\n",
            "[0.00s -> 3.80s]  Which three NBA teams have the worst home team win record?\n",
            " Which three NBA teams have the worst home team win record?.\n"
          ]
        }
      ],
      "source": [
        "## CODE TO READ IN MP3 AUDIO QUERY FROM USER\n",
        "\n",
        "#MALE PERSON ASKING THE FOLLOWING QUESTION IN ARABIC\n",
        "# #I am visiting Dubai for 3 days. Please suggest suggest places to see for my Dubai visit.\n",
        "##I am visiting Dubai for 3 days. Please suggest suggest places to see for my Dubai visit.\n",
        "#inputtext = \"\"\"سأزور دبي لمدة 3 أيام. يرجى اقتراح أماكن يمكن رؤيتها خلال زيارتي لدبي.\"\"\"\n",
        "#segments2, info2 = model.transcribe(\"male-arabic-mp3-dubaivisitquestion3days.mp3\", beam_size=5)\n",
        "\n",
        "#Which are the five NBA teams with the lowest average percentage of 2 point scores in the NBA ? List the names and their statistics\n",
        "#segments2, info2 = modelwhisper.transcribe(\"nba-question_5worstavg_mp3.mp3\", beam_size=5)\n",
        "\n",
        "#nba-question_7mostwins_mp3\n",
        "#Which are the seven teams with most wins  at home ? List the names and their statistics\n",
        "#segments2, info2 = modelwhisper.transcribe(\"nba-question_7mosttotalwins_mp3.mp3\", beam_size=5)\n",
        "#segments2, info2 = modelwhisper.transcribe(\"nba-question_5worstavg_mp3.mp3\", beam_size=5)\n",
        "\n",
        "#QUESTION 1 - TOP 5 REBOUNDING\n",
        "#Which five NBA teams are the best at rebounding?\n",
        "#segments2, info2 = modelwhisper.transcribe(\"question1_top5rebounding.mp3\", beam_size=5)\n",
        "\n",
        "#QUESTION 2 - compare 76er vs gsw\n",
        "#Provide an explanation for which NBA team shoots better 3 pointers: the Philadelphia 76ers or the Golden State Warriors?.\n",
        "#segments2, info2 = modelwhisper.transcribe(\"question2_compare76er_vs_GSW.mp3\", beam_size=5)\n",
        "\n",
        "#QUESTION 3 - worst 3 home team win record - female child voice\n",
        "##DEMO 3-2\n",
        "#user_question = \"Which three NBA teams have the worst home team win record?\"\n",
        "segments2, info2 = modelwhisper.transcribe(\"question3_worstwinrecord.mp3\", beam_size=5)\n",
        "\n",
        "\n",
        "##Spanish text\n",
        "#I am visiting Dubai for 3 days. Please suggest suggest places to see for my Dubai visit.\n",
        "#Voy a visitar Dubái durante 3 días. ¿Podrías sugerirme lugares para ver durante mi visita a Dubái?\n",
        "#Voy a visitar Dubái durante 3 días. ¿Podrías sugerirme lugares para ver durante mi visita a Dubái?\n",
        "#segments2, info2 = model.transcribe(\"nba-question_5worstavg_mp3\") #male-spanish-mp3-dubaivisitquestion3days.mp3\", beam_size=5)\n",
        "\n",
        "print(\"Detected language '%s' with probability %f\" % (info2.language, info2.language_probability))\n",
        "\n",
        "segmentvoiceuser = \"\"\n",
        "for segment in segments2:\n",
        "    print(\"[%.2fs -> %.2fs] %s\" % (segment.start, segment.end, segment.text))\n",
        "    segmentvoiceuser = segmentvoiceuser + segment.text + \".\"\n",
        "\n",
        "# NOTE THAT THE WHISPER MODEL DOES A GREAT JOB OF UNDERSTANDING THERE ARE 2 SENTENCES IN QUESTION\n",
        "#AND SPOTTING THE PAUSE\n",
        "print(segmentvoiceuser)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57ad6c4f-9823-4906-9715-15f0ae6c2394",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-09-10T22:40:07.497328Z",
          "iopub.status.busy": "2024-09-10T22:40:07.496885Z",
          "iopub.status.idle": "2024-09-10T22:40:11.087291Z",
          "shell.execute_reply": "2024-09-10T22:40:11.086186Z",
          "shell.execute_reply.started": "2024-09-10T22:40:07.497295Z"
        },
        "id": "57ad6c4f-9823-4906-9715-15f0ae6c2394",
        "outputId": "290eae28-ec54-4653-d7da-242312f9620d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: 17 prefix-match hit, remaining 1125 prompt tokens to eval\n",
            "\n",
            "llama_print_timings:        load time =     786.22 ms\n",
            "llama_print_timings:      sample time =       5.27 ms /    40 runs   (    0.13 ms per token,  7593.01 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1902.70 ms /  1125 tokens (    1.69 ms per token,   591.26 tokens per second)\n",
            "llama_print_timings:        eval time =    1580.97 ms /    39 runs   (   40.54 ms per token,    24.67 tokens per second)\n",
            "llama_print_timings:       total time =    3569.85 ms /  1164 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'id': 'cmpl-d98f62de-b88c-4e90-8d9d-d6df4f840d41', 'object': 'text_completion', 'created': 1726008007, 'model': '/root/.cache/huggingface/hub/models--bartowski--llama-3-sqlcoder-8b-GGUF/snapshots/57201bd362a1270614a15458f7aa3c3ed7826b6e/llama-3-sqlcoder-8b-Q5_K_M.gguf', 'choices': [{'text': \"SELECT n.team_name_home, COUNT(*) AS loss_count FROM nbatest n WHERE n.wl_home = 'L' GROUP BY n.team_name_home ORDER BY loss_count DESC LIMIT 3;\", 'index': 0, 'logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 1142, 'completion_tokens': 39, 'total_tokens': 1181}}\n",
            "SELECT n.team_name_home, COUNT(*) AS loss_count FROM nbatest n WHERE n.wl_home = 'L' GROUP BY n.team_name_home ORDER BY loss_count DESC LIMIT 3;\n"
          ]
        }
      ],
      "source": [
        "user_question = segmentvoiceuser\n",
        "nbaquestion = user_question\n",
        "prompt = f\"\"\"\n",
        "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
        "\n",
        "Generate a SQL query to answer this question: `{user_question}` Do not generate ANY extra output except for the SQL query.\n",
        "You are a data analyst and interpreter. You will be given information about a specific NBA dataset, including its columns and their meanings:\n",
        "\n",
        "  team_id_home  number:       Column represents Unique identifier for the NBA team\n",
        "  team_abbreviation_home text:  Column represents Abbreviated Name of NBA team\n",
        "  team_name_home text:       Column represents Full Name of NBA team\n",
        "  game_date time:                Column represents Date of the game\n",
        "  wl_home text:            Column represents  Single game Win (W) or Loss (L) by the home team\n",
        "  fgm_home number:     Column represents  2 point field goals or 2 pointers succesfully made by home team\n",
        "  fga_home number:      Column represents  2 point field goals or 2 pointers attempted by home team\n",
        "  fg_pct_home number:  Column represents  Succesful ratio of 2 point field goals or 2 pointers made by home team\n",
        "  fg3m_home:          Column represents  3 point field goals or 2 pointers succesfully made by home team\n",
        "  fg3a_home:            Column represents 3 point field goals or 3 pointers attempted by home team\n",
        "  fg3_pct_home,:      Column represents Succesful ratio of 3 point field goals or 3 pointers made by home team\n",
        "  ftm_home:           Column represents free throws succesfully made by home team\n",
        "  fta_home:            Column represents free throws attempted by home team\n",
        "  ft_pct_home:          Column represents Succesful ratio of free throws made by home team\n",
        "  oreb_home:            Column represents offensive rebounds by home team\n",
        "  dreb_home:           Column represents defensive rebounds by home team\n",
        "  reb_home:            Column represents Total rebounds by home team\n",
        "  ast_home:             Column represents assists by home team\n",
        "  stl_home:             Column represents steals by home team\n",
        "  blk_home:            Column represents blocks by home team\n",
        "  tov_home:            Column represents turnovers by home team\n",
        "  pf_home:              Column represents personal fouls by home team\n",
        "  pts_home:             Column represents total points in game made by home team\n",
        "  plus_minus_home :      Column represents points differential of game win (+) or loss (-) by home team\n",
        "\n",
        "If you cannot answer the question with the available database schema, return 'I do not know'\n",
        "\n",
        "Using this information, you will set up the most suitable SQL statements to answer questions related to the data. These SQL statements will later be executed, so they should be error-free, compatible with Sqllite syntax, and accurately respond to the questions asked. Do not express an opinion or try to explain. Return only the SQL statement. Remember your sql statement will run on SQLite so syntax should be correct. Your output should be clear and void like this output 'Here is the SQL statement to answer the question:'. Also, do not add any comment to the SQL statement which you will generate. Only return SQL statement.\n",
        "\n",
        "DDL statements:\n",
        "CREATE TABLE nbatest (\n",
        "  team_id_home  INTEGER PRIMARY KEY, -- Unique identifier for the NBA team\n",
        "  team_abbreviation_home text,   -- Abbreviated Name of NBA team\n",
        "  team_name_home VARCHAR(50),   -- Full Name of NBA team\n",
        "  game_date DATE,                 -- Date of the game\n",
        "  wl_home VARCHAR(50),                -- Single game Win (W) or Loss (L) by the home team\n",
        "  fgm_home INTEGER,     -- Combined 2 and 3 point field goals or 2 and 3 pointers succesfully made by home team\n",
        "  fga_home INTEGER,          -- Combined 2 point field goals or 2 and 3 pointers attempted by home team\n",
        "  fg_pct_home DECIMAL(3,2),       -- Succesful ratio of 2 point field goals or 2 pointers made by home team\n",
        "  fg3m_home INTEGER,             -- 3 point field goals or 2 pointers succesfully made by home team\n",
        "  fg3a_home INTEGER,             -- 3 point field goals or 3 pointers attempted by home team\n",
        "  fg3_pct_home DECIMAL(3,2),          -- Succesful ratio of 3 point field goals or 3 pointers made by home team\n",
        "  ftm_home INTEGER,              -- free throws succesfully made by home team\n",
        "  fta_home INTEGER,              -- free throws attempted by home team\n",
        "  ft_pct_home DECIMAL(3,2),           -- Succesful ratio of free throws made by home team\n",
        "  oreb_home INTEGER,             -- offensive rebounds by home team\n",
        "  dreb_home INTEGER,             -- defensive rebounds by home team\n",
        "  reb_home INTEGER,              -- Total rebounds by home team\n",
        "  ast_home INTEGER,              -- assists by home team\n",
        "  stl_home INTEGER,              -- steals by home team\n",
        "  blk_home INTEGER,              -- blocks by home team\n",
        "  tov_home INTEGER,              -- turnovers by home team\n",
        "  pf_home INTEGER,               -- personal fouls by home team\n",
        "  pts_home INTEGER,              -- total points in game made by home team\n",
        "  plus_minus_home INTEGER        -- points differential of game win (+) or loss (-) by home team\n",
        ")<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
        "\n",
        "The following SQL query best answers the question `{user_question}`:\n",
        "```sql\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "output = llmsql(\n",
        "      prompt, # Prompt\n",
        "      max_tokens=168, # Generate up to 32 tokens, set to None to generate up to the end of the context window\n",
        "      #stop=[\"Q:\", \"\\n\"], # Stop generating just before the model would generate a new question\n",
        "      #echo=True # Echo the prompt back in the output\n",
        ") # Generate a completion, can also call create_completion\n",
        "print(output)\n",
        "\n",
        "#out2=(output['choices'])\n",
        "\n",
        "#out2=(output['choices'])\n",
        "#print(out2[0]['text'])\n",
        "print(output['choices'][0]['text'])\n",
        "\n",
        "sqlexec = output['choices'][0]['text']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7f1c0c2-e575-44b9-84da-8095c2c1da35",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-09-10T22:40:17.112624Z",
          "iopub.status.busy": "2024-09-10T22:40:17.111544Z",
          "iopub.status.idle": "2024-09-10T22:40:17.142117Z",
          "shell.execute_reply": "2024-09-10T22:40:17.140918Z",
          "shell.execute_reply.started": "2024-09-10T22:40:17.112599Z"
        },
        "id": "c7f1c0c2-e575-44b9-84da-8095c2c1da35",
        "outputId": "eb768666-be23-40db-c838-b264a92dc0ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('nbatest',)]\n",
            "SELECT n.team_name_home, COUNT(*) AS loss_count FROM nbatest n WHERE n.wl_home = 'L' GROUP BY n.team_name_home ORDER BY loss_count DESC LIMIT 3;\n",
            "[('Detroit Pistons', 136), ('Charlotte Hornets', 124), ('San Antonio Spurs', 120)]\n",
            "[('Detroit Pistons', 136), ('Charlotte Hornets', 124), ('San Antonio Spurs', 120)]\n",
            "(['Detroit Pistons', 136], ['Charlotte Hornets', 124], ['San Antonio Spurs', 120])\n"
          ]
        }
      ],
      "source": [
        "import sqlite3 as lite\n",
        "\n",
        "conn = lite.connect('nbatest.db')\n",
        "cur = conn.cursor()\n",
        "\n",
        "# reading all table names\n",
        "table_list = [a for a in cur.execute(\"SELECT name FROM sqlite_master WHERE type = 'table'\")]\n",
        "# here is you table list\n",
        "print(table_list)\n",
        "\n",
        "# Be sure to close the connection\n",
        "#conn.close()\n",
        "\n",
        "print(sqlexec)\n",
        "res=cur.execute(sqlexec)\n",
        "\n",
        "#res = cur.execute(\"SELECT name FROM sqlite_master\")\n",
        "#res.fetchall()\n",
        "\n",
        "nbasqlanswer = res.fetchall()\n",
        "\n",
        "print(nbasqlanswer)\n",
        "\n",
        "res22=list(cur.execute(sqlexec))\n",
        "print(res22)\n",
        "\n",
        "\n",
        "##  **BELOW CLEAN UP THE SQL OUTPUT AND ONLY PRINT FOLATING POINT RESULTS UPTO 2 DECIMAL PLACES\n",
        "\n",
        "def is_decimal(element):\n",
        "    try:\n",
        "        float(element)\n",
        "        return True\n",
        "    except ValueError:\n",
        "        return False\n",
        "\n",
        "my_tuple = res22\n",
        "resrounded = res22\n",
        "\n",
        "# convert tuple to list and back to tuple\n",
        "#https://stackoverflow.com/questions/77350268/tuple-object-does-not-support-item-assignment-on-array\n",
        "\n",
        "my_list = [list(x) for x in res22]  # Convert each tuple to a list\n",
        "\n",
        "for x in range(0,len(my_list)):\n",
        "    #sip = my_list[y][0].split(\"/\")\n",
        "    #my_list[y][0]=sip[0]\n",
        "  for y in range(len(my_list[x])):\n",
        "    val = my_list[x][y]\n",
        "    if is_decimal(val):\n",
        "        #print(f\"{val} is a decimal.\")\n",
        "        my_list[x][y] = round(val,2)\n",
        "    #else:\n",
        "        #print(f\"{val} is not a decimal.\")\n",
        "\n",
        "\n",
        "myary = tuple(my_list)  # Convert them back to a tuple of tuples\n",
        "print(myary)\n",
        "\n",
        "nbaroundedanswer = myary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c06e679a-8a7a-42a8-8b4e-698e63be6f34",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-09-10T22:41:11.147876Z",
          "iopub.status.busy": "2024-09-10T22:41:11.147047Z",
          "iopub.status.idle": "2024-09-10T22:41:11.156896Z",
          "shell.execute_reply": "2024-09-10T22:41:11.155869Z",
          "shell.execute_reply.started": "2024-09-10T22:41:11.147850Z"
        },
        "id": "c06e679a-8a7a-42a8-8b4e-698e63be6f34",
        "outputId": "dfe75eec-379c-4a8d-877d-6ada0a52a04f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(['Detroit Pistons', 136], ['Charlotte Hornets', 124], ['San Antonio Spurs', 120])\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def is_decimal(element):\n",
        "    try:\n",
        "        float(element)\n",
        "        return True\n",
        "    except ValueError:\n",
        "        return False\n",
        "\n",
        "my_tuple = res22\n",
        "resrounded = res22\n",
        "\n",
        "# convert tuple to list and back to tuple\n",
        "#https://stackoverflow.com/questions/77350268/tuple-object-does-not-support-item-assignment-on-array\n",
        "\n",
        "my_list = [list(x) for x in res22]  # Convert each tuple to a list\n",
        "\n",
        "for x in range(0,len(my_list)):\n",
        "    #sip = my_list[y][0].split(\"/\")\n",
        "    #my_list[y][0]=sip[0]\n",
        "  for y in range(len(my_list[x])):\n",
        "    val = my_list[x][y]\n",
        "    if is_decimal(val):\n",
        "        #print(f\"{val} is a decimal.\")\n",
        "        my_list[x][y] = round(val,2)\n",
        "    #else:\n",
        "        #print(f\"{val} is not a decimal.\")\n",
        "\n",
        "\n",
        "myary = tuple(my_list)  # Convert them back to a tuple of tuples\n",
        "print(myary)\n",
        "\n",
        "nbaroundedanswer = myary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0cce46c0-c710-4a58-b8bd-ee2e5ddf06b3",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-09-10T22:41:17.852416Z",
          "iopub.status.busy": "2024-09-10T22:41:17.851113Z",
          "iopub.status.idle": "2024-09-10T22:41:18.787027Z",
          "shell.execute_reply": "2024-09-10T22:41:18.785866Z",
          "shell.execute_reply.started": "2024-09-10T22:41:17.852375Z"
        },
        "id": "0cce46c0-c710-4a58-b8bd-ee2e5ddf06b3",
        "outputId": "7eeb3a44-5b82-40d1-f921-00df6931c48d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(['Detroit Pistons', 136], ['Charlotte Hornets', 124], ['San Antonio Spurs', 120])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: 164 prefix-match hit, remaining 1 prompt tokens to eval\n",
            "\n",
            "llama_print_timings:        load time =     121.91 ms\n",
            "llama_print_timings:      sample time =      12.24 ms /    52 runs   (    0.24 ms per token,  4248.37 tokens per second)\n",
            "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
            "llama_print_timings:        eval time =     772.68 ms /    52 runs   (   14.86 ms per token,    67.30 tokens per second)\n",
            "llama_print_timings:       total time =     919.34 ms /    52 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'id': 'cmpl-a890022c-0a29-41e5-80e1-f2bc84f3d9a7', 'object': 'text_completion', 'created': 1726008077, 'model': '/root/.cache/huggingface/hub/models--MaziyarPanahi--gemma-2b-it-GGUF/snapshots/72164ae6fc4003cecf37bc07f3a825b8a20b8cbb/gemma-2b-it.Q8_0.gguf', 'choices': [{'text': '\\n\\nIn response to your question, the three NBA teams with the worst home team win record are:\\n\\n- Detroit Pistons with 136 losses\\n- Charlotte Hornets with 124 losses\\n- San Antonio Spurs with 120 losses', 'index': 0, 'logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 165, 'completion_tokens': 51, 'total_tokens': 216}}\n",
            "\n",
            "\n",
            "In response to your question, the three NBA teams with the worst home team win record are:\n",
            "\n",
            "- Detroit Pistons with 136 losses\n",
            "- Charlotte Hornets with 124 losses\n",
            "- San Antonio Spurs with 120 losses\n",
            "Model Answer : \n",
            " \n",
            "\n",
            "In response to your question, the three NBA teams with the worst home team win record are\n",
            "\n",
            "- Detroit Pistons with 136 losses\n",
            "- Charlotte Hornets with 124 losses\n",
            "- San Antonio Spurs with 120 losses\n"
          ]
        }
      ],
      "source": [
        "nbaquestion = user_question\n",
        "\n",
        "print(nbaroundedanswer)\n",
        "prompt = f\"\"\"<start_of_turn>userThe following is an answer extracted from a SQL Database to a user question:\n",
        "        Question: {nbaquestion}\n",
        "       Extracted Answer: {nbaroundedanswer}\n",
        "       Please simulate a spoken response. Say \"In response to your question\", and then rephrase the question briefly, and answer the question as a narrartive using only the information from the answer in the list format above. Include statistical and numbers if theere are list or tuple in the SQL response. Do not use other additional information\n",
        "       <end_of_turn>\n",
        "       #<start_of_turn>model\"\"\"\n",
        "\n",
        "output = llm(\n",
        "      prompt, # Prompt\n",
        "      max_tokens=168, # Generate up to 32 tokens, set to None to generate up to the end of the context window\n",
        "      #stop=[\"Q:\", \"\\n\"], # Stop generating just before the model would generate a new question\n",
        "      #echo=True # Echo the prompt back in the output\n",
        ") # Generate a completion, can also call create_completion\n",
        "print(output)\n",
        "\n",
        "#out2=(output['choices'])\n",
        "\n",
        "#out2=(output['choices'])\n",
        "#print(out2[0]['text'])\n",
        "finalnballmresponse = (output['choices'][0]['text'])\n",
        "\n",
        "print(finalnballmresponse)\n",
        "\n",
        "import re\n",
        "#s = re.sub(r\"[!\\\"#$%&\\'()*+,/:;<=>?@[\\]^_`{|}]\",\"\",s)\n",
        "#s = re.sub(r\"[-()\\\"#/@;:<>{}`+=~|.!?,]\", \"\", s)\n",
        "\n",
        "#retain . , ? ! $ , but remove * #\n",
        "finalcleannballmresponse = re.sub(r\"[()\\\"#*/@;:<>{}`+=~|]\", \"\", finalnballmresponse)\n",
        "\n",
        "print(f\"Model Answer : \\n {finalcleannballmresponse}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc3b1dcb-98e5-4bac-acca-bce1cdc7dfb8",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-09-10T22:42:52.314296Z",
          "iopub.status.busy": "2024-09-10T22:42:52.313893Z",
          "iopub.status.idle": "2024-09-10T22:42:53.554424Z",
          "shell.execute_reply": "2024-09-10T22:42:53.552804Z",
          "shell.execute_reply.started": "2024-09-10T22:42:52.314263Z"
        },
        "id": "bc3b1dcb-98e5-4bac-acca-bce1cdc7dfb8",
        "outputId": "f0270506-de4f-48ce-d92b-ecb7cfa5b293"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "In response to your question, the three NBA teams with the worst home team win record are\n",
            "\n",
            "- Detroit Pistons with 136 losses\n",
            "- Charlotte Hornets with 124 losses\n",
            "- San Antonio Spurs with 120 losses\n"
          ]
        }
      ],
      "source": [
        "\n",
        "#!pip install gTTS\n",
        "\n",
        "## SAVE THE LLM RESPONSE INTO AN MP3 AUDIO FILE\n",
        "print(finalcleannballmresponse)\n",
        "from gtts import gTTS\n",
        "tts = gTTS(finalcleannballmresponse,lang='en')\n",
        "#tts.save('llmspokenresponse_arabic_fromarabicquestion_noxlate.mp3')\n",
        "\n",
        "#tts.save('llmspokenresponse_nbaquestion_mostwinshome.mp3')\n",
        "\n",
        "#tts.save('llmanswer1_top5rebounding.mp3')\n",
        "\n",
        "#tts.save('llmanswer2_compare76er_vs_GSW.mp3')\n",
        "\n",
        "tts.save('llmanswer3_worstwinrecord.mp3')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c41e92b5-cfe5-4d64-9dee-4c2de011bc50",
      "metadata": {
        "id": "c41e92b5-cfe5-4d64-9dee-4c2de011bc50"
      },
      "outputs": [],
      "source": [
        "####DONE WITH NBA QUESTIONS - REST OF THS NOTEBOOK IS OLD CODE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "428bc3b1-3a10-4fa6-b28a-d54df3e7ea75",
      "metadata": {
        "id": "428bc3b1-3a10-4fa6-b28a-d54df3e7ea75"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}